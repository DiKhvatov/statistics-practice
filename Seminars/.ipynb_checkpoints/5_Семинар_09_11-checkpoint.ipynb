{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IxoPzBXPkjZD"
   },
   "source": [
    "# Курс \"Практикум по математической статистике\"\n",
    "# 3 курс ФПМИ МФТИ, осень 2024\n",
    "## Семинар 09.11. Регрессия. Отбор признаков.\n",
    "\n",
    "Этот ноутбук представляет собой материал для семинара \"Регрессия и отбор признаков\". Читая его, Вы научитесь:\n",
    "\n",
    "- Строить регрессионные модели\n",
    "\n",
    "- Сравнивать модели между собой (без \"преференций\" в сторону сложных моделей)\n",
    "\n",
    "- Оценивать полезность признака численно и принимать решение о необходимости его удаления\n",
    "\n",
    "- Вероятностно интерпретировать понятие регуляризации и переобучения\n",
    "\n",
    "Вопросы по ноутбуку и первому домашнему заданию задавайте в телеграмм автору https://t.me/vitalii_kondratiuk.\n",
    "\n",
    "Sapere aude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TxK5OZ7NkGQz",
    "outputId": "324d46b9-6ffd-4cfa-f55d-ed009fa0026a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (3.9.2)\n",
      "Requirement already satisfied: numpy in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (2.1.3)\n",
      "Requirement already satisfied: scipy in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (1.14.1)\n",
      "Requirement already satisfied: statsmodels in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (0.14.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from statsmodels) (2.2.3)\n",
      "Requirement already satisfied: patsy>=0.5.6 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from statsmodels) (0.5.6)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2023.3)\n",
      "Requirement already satisfied: six in /opt/miniconda3/envs/Stats/lib/python3.13/site-packages (from patsy>=0.5.6->statsmodels) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade matplotlib numpy scipy statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "yseVbUtrliRz"
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import typing\n",
    "import abc\n",
    "import dataclasses\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sps\n",
    "import seaborn as sns\n",
    "import statsmodels.api as sm\n",
    "import scipy as sp\n",
    "\n",
    "sns.set(style='darkgrid')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQSqLePvloGe"
   },
   "source": [
    "Терминология:\n",
    "\n",
    "- Задачей регрессии называется восстановление параметров условной плотности $P(Y | X)$, считая что $Y = w^TX + ɛ$, где последнее слагаемое --- гауссовский шум.\n",
    "- В курсе математической статистики Вы привыкли иметь дело с генеративной версией этой модели: оцениваем $X$ по выборке $X_i$, где $X_i = Z + ɛ$, $Z \\in L$ --- линейное подпространство\n",
    "- По факту это одна и та же задача, так что все выкладки верны в обоих моделях. Мы будем работать с более привычной для практики первой моделью.\n",
    "- Считается что $X = (x_1,...,x_n)$, где $x_i$ называется **признаком**. Выборку $X$ называют **матрицей объект-признак**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "00TElHuMpKcs"
   },
   "source": [
    "1. Как выглядит правдоподобие и оптимальная оценка параметров?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmAKzRYZpT7b"
   },
   "source": [
    "$Y \\sim N(w^TX, \\beta^{-1}I)$, где $I$ --- единичная матрица. Можно посчитать правдоподобие по $w$ и найти его минимум.\n",
    "\n",
    "Тогда $w^* = argmin ||Y - w^TX||^2$.\n",
    "\n",
    "Существует аналитическое решение этой задачи $w^* = (X^TX)^{-1}X^TY$. На практике его редко ищут, предпочитая решать задачу стохастическим градиентным спуском.\n",
    "\n",
    "Изучим свойства $w^*$:\n",
    " - В каком случае матрица $(X^TX)$ необратима?\n",
    " - В каком случае значение $w^*$ существенно меняется от зашумления $Y$?\n",
    " - Как улучшить свойства матрицы объектов-признаков?\n",
    " - Часто в регрессии любят оптимизировать не сумму квадратов, а сумму модулей. Как выглядит вероятностная модель, которая приводит к такой задаче оптимизации?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SE27-bZvu-Gl"
   },
   "source": [
    "Подготовим модельные данные. Предлагается предсказать сумму страховых выплат человека по каким-то анкетным параметрам. Пока просто посмотрим на задачу с точки зрения статистики, игнорируя смысл параметров, а после проведения формального отбора признаков проинтерпретируем результат."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "60yx6y9wlkRO",
    "outputId": "e4bec0a5-c216-4af9-ad2b-3ce0984dc528"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-11-26 02:40:59--  https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv\n",
      "R'esolution de raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
      "Connexion `a raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connect'e.\n",
      "requ^ete HTTP transmise, en attente de la r'eponse... 200 OK\n",
      "Taille : 54288 (53K) [text/plain]\n",
      "Sauvegarde en : <<insurance.csv.1>>\n",
      "\n",
      "insurance.csv.1     100%[===================>]  53.02K  --.-KB/s    ds 0.04s   \n",
      "\n",
      "2024-11-26 02:40:59 (1.15 MB/s) - <<insurance.csv.1>> sauvegard'e [54288/54288]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/stedy/Machine-Learning-with-R-datasets/master/insurance.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GUhWy4sWvUs8"
   },
   "source": [
    "Подготовим данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DHkJn-TDtDcg",
    "outputId": "612947c3-b8af-49e4-cfad-7c97542b9f4d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1338, 12), (1338,))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"insurance.csv\")\n",
    "data = pd.get_dummies(data = data, columns = [\"sex\", \"region\", \"smoker\"])\n",
    "data[\"constant\"] = 1\n",
    "Y = data[\"charges\"].to_numpy().astype(float)\n",
    "X = data.drop('charges', axis=1).to_numpy().astype(float)\n",
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 443
    },
    "id": "ohsHwqD3tYTS",
    "outputId": "35cdc4dd-ed8f-45ff-cd19-c0fbf70d699e"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>bmi</th>\n",
       "      <th>children</th>\n",
       "      <th>charges</th>\n",
       "      <th>sex_female</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>region_northeast</th>\n",
       "      <th>region_northwest</th>\n",
       "      <th>region_southeast</th>\n",
       "      <th>region_southwest</th>\n",
       "      <th>smoker_no</th>\n",
       "      <th>smoker_yes</th>\n",
       "      <th>constant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>19</td>\n",
       "      <td>27.900</td>\n",
       "      <td>0</td>\n",
       "      <td>16884.92400</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18</td>\n",
       "      <td>33.770</td>\n",
       "      <td>1</td>\n",
       "      <td>1725.55230</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>33.000</td>\n",
       "      <td>3</td>\n",
       "      <td>4449.46200</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33</td>\n",
       "      <td>22.705</td>\n",
       "      <td>0</td>\n",
       "      <td>21984.47061</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>32</td>\n",
       "      <td>28.880</td>\n",
       "      <td>0</td>\n",
       "      <td>3866.85520</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1333</th>\n",
       "      <td>50</td>\n",
       "      <td>30.970</td>\n",
       "      <td>3</td>\n",
       "      <td>10600.54830</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334</th>\n",
       "      <td>18</td>\n",
       "      <td>31.920</td>\n",
       "      <td>0</td>\n",
       "      <td>2205.98080</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1335</th>\n",
       "      <td>18</td>\n",
       "      <td>36.850</td>\n",
       "      <td>0</td>\n",
       "      <td>1629.83350</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1336</th>\n",
       "      <td>21</td>\n",
       "      <td>25.800</td>\n",
       "      <td>0</td>\n",
       "      <td>2007.94500</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337</th>\n",
       "      <td>61</td>\n",
       "      <td>29.070</td>\n",
       "      <td>0</td>\n",
       "      <td>29141.36030</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1338 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      age     bmi  children      charges  sex_female  sex_male  \\\n",
       "0      19  27.900         0  16884.92400        True     False   \n",
       "1      18  33.770         1   1725.55230       False      True   \n",
       "2      28  33.000         3   4449.46200       False      True   \n",
       "3      33  22.705         0  21984.47061       False      True   \n",
       "4      32  28.880         0   3866.85520       False      True   \n",
       "...   ...     ...       ...          ...         ...       ...   \n",
       "1333   50  30.970         3  10600.54830       False      True   \n",
       "1334   18  31.920         0   2205.98080        True     False   \n",
       "1335   18  36.850         0   1629.83350        True     False   \n",
       "1336   21  25.800         0   2007.94500        True     False   \n",
       "1337   61  29.070         0  29141.36030        True     False   \n",
       "\n",
       "      region_northeast  region_northwest  region_southeast  region_southwest  \\\n",
       "0                False             False             False              True   \n",
       "1                False             False              True             False   \n",
       "2                False             False              True             False   \n",
       "3                False              True             False             False   \n",
       "4                False              True             False             False   \n",
       "...                ...               ...               ...               ...   \n",
       "1333             False              True             False             False   \n",
       "1334              True             False             False             False   \n",
       "1335             False             False              True             False   \n",
       "1336             False             False             False              True   \n",
       "1337             False              True             False             False   \n",
       "\n",
       "      smoker_no  smoker_yes  constant  \n",
       "0         False        True         1  \n",
       "1          True       False         1  \n",
       "2          True       False         1  \n",
       "3          True       False         1  \n",
       "4          True       False         1  \n",
       "...         ...         ...       ...  \n",
       "1333       True       False         1  \n",
       "1334       True       False         1  \n",
       "1335       True       False         1  \n",
       "1336       True       False         1  \n",
       "1337      False        True         1  \n",
       "\n",
       "[1338 rows x 13 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jFKjfMldNZAw",
    "outputId": "5ff65fd3-f4aa-4cd4-c4a6-c66da9058657"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['age', 'bmi', 'children', 'sex_female', 'sex_male', 'region_northeast',\n",
       "       'region_northwest', 'region_southeast', 'region_southwest', 'smoker_no',\n",
       "       'smoker_yes', 'constant'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.drop('charges', axis=1).keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "h_qXX7csvYas"
   },
   "outputs": [],
   "source": [
    "w1 = sp.linalg.pinv(X)@Y\n",
    "w2 = np.linalg.inv(X.T@X)@X.T@Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "URf3wL94vYUw",
    "outputId": "2c4227e4-249a-4d51-e619-ee087de69fd0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6041.6796511744515, 32986.32396155326)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((Y - X@w1)**2)), np.sqrt(np.mean((Y - X@w2)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wO3jXUtCFlT_"
   },
   "source": [
    "Заметим, что перепады качества тут исключительно из-за численных ошибок обращения матриц. Как с этим бороться вы узнаете на курсе выч. математики, а мы сравнимся с регрессией из коробки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lIwibpf_vnaM",
    "outputId": "5431691f-aaf7-4a84-8477-631781df127e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.751\n",
      "Model:                            OLS   Adj. R-squared:                  0.749\n",
      "Method:                 Least Squares   F-statistic:                     500.8\n",
      "Date:                Sat, 09 Nov 2024   Prob (F-statistic):               0.00\n",
      "Time:                        09:22:11   Log-Likelihood:                -13548.\n",
      "No. Observations:                1338   AIC:                         2.711e+04\n",
      "Df Residuals:                    1329   BIC:                         2.716e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1           256.8564     11.899     21.587      0.000     233.514     280.199\n",
      "x2           339.1935     28.599     11.860      0.000     283.088     395.298\n",
      "x3           475.5005    137.804      3.451      0.001     205.163     745.838\n",
      "x4           -82.5512    269.226     -0.307      0.759    -610.706     445.604\n",
      "x5          -213.8656    274.976     -0.778      0.437    -753.299     325.568\n",
      "x6           512.9050    300.348      1.708      0.088     -76.303    1102.113\n",
      "x7           159.9411    301.334      0.531      0.596    -431.201     751.083\n",
      "x8          -522.1170    330.759     -1.579      0.115   -1170.983     126.749\n",
      "x9          -447.1459    310.933     -1.438      0.151   -1057.119     162.827\n",
      "x10        -1.207e+04    282.338    -42.759      0.000   -1.26e+04   -1.15e+04\n",
      "x11         1.178e+04    313.530     37.560      0.000    1.12e+04    1.24e+04\n",
      "const       -296.4168    430.507     -0.689      0.491   -1140.964     548.130\n",
      "==============================================================================\n",
      "Omnibus:                      300.366   Durbin-Watson:                   2.088\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              718.887\n",
      "Skew:                           1.211   Prob(JB):                    7.86e-157\n",
      "Kurtosis:                       5.651   Cond. No.                     5.40e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 1.21e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "mod = sm.OLS(Y, X)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1VLpcpG9zB_O",
    "outputId": "abb67edc-2cb2-4b54-a7c7-0cf8baf89327"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16088.97327007743"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((Y - mod.predict(X.T))**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xxF1QBOYFwVr"
   },
   "source": [
    "Как читать саммари выше?\n",
    "Что такое $R^2$ мы ещё поговорим, когда дойдём до оценок качества, там же вспомним про AIC и BIC. Что такое $F$-статистика вам ещё расскажет [Данные удалены] на семинаре про проверку гипотез, а пока скажем, что это некоторая мера значимости модели. Если её вероятность (Prob (F-statistic)) меньше 0.05, то можно делать вывод, что модель хоть что-то объясняет. В противном случае такой вывод делать нельзя и все фичи бесполезны.\n",
    "\n",
    "В таблице ниже для каждой фичи мы можем видеть её значение и её дисперсию, а также значение $t-$критерия и его значимость. Пока смотрим на это так: если $P > |t|$ это число меньше, чем 0.05, то признак важен, а если больше, то нет. Неудивительно, что модель ругается на one-hot переменные, которые ещё и линейно зависимы (так как мы намеренно не убрали лишние). Ниже нам даже об этом сообщили --- у нас есть мультиколлинеарность."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DutHq8KCHkWx"
   },
   "source": [
    "Заметим, что в задаче отбора признаков мы натыкаемся на типичную проблему: так как \"простая\" модель есть частный случай \"сложной\" модели, то метрики при добавлении признаков будут только расти (считая, что нет численных ошибок, конечно). Метрики будут расти как при добавлении информативных признаков, так и при добавлении почти шумовых. На практике с этим борятся при помощи валидационной выборки и кросс-валидации, о чём вам расскажут на курсе машинного обучения, а мы подойдём к задаче со стороны чистой статистики."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "looGp9yiIObH"
   },
   "source": [
    "1. Попробуем оценить как много информации модель узнала о выборке. Для этого будем считать, что наблюдается закон природы $Y = w^TX$, но наблюдения зашумлены некоторым $\\varepsilon`$.\n",
    "\n",
    "Допустим, мы вообще не знаем никаких признаков и считаем $Y$ обычной нормальной величиной. Мерой её \"случайности\" считают её дисперсию $\\sigma_t^2$. Действительно, в случае отстутсвия фичей лучшим предсказанием $Y$ будет его мат. ожидание, а точность такого предсказания это дисперсия.\n",
    "\n",
    "Теперь нам дали какие-то фичи $X$. Мы подобрали по методу максимального правдоподобия параметры $w$ и оцениваем $Y = w^TX$. Заметим, что наше оценивание всё ещё неточное, и мера этой неточности оценивается как $D[Y|X] = \\sigma^2$. Это называется **необъяснённая дисперсия**. В противовес, $D[w^TX] = \\sigma_y^2$ называется **объяснённая дисперсия**. Нетрудно проверить, что общая дисперсия раскладывается в сумму $\\sigma_t^2 = \\sigma_y^2 + \\sigma^2$.\n",
    "\n",
    "Оценим эти величины:\n",
    "\n",
    "$\\sigma_t^2 = \\frac{1}{n}\\sum (y - \\overline{y})^2$\n",
    "\n",
    "$\\sigma_y^2 = \\frac{1}{n}\\sum (\\hat{y} - \\overline{y})^2$, где $\\hat{y} = w^TX$\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{n}\\sum (\\hat{y} - y)^2$\n",
    "\n",
    "Коэффициентом детерминации $R^2$ называют долю объяснённой дисперсии, но считают обычно $R^2 = 1 - \\frac{\\sigma^2}{\\sigma_t^2}$. На практике значение $R^2$ ниже 50% считается плохим, а выше 80% хорошим.\n",
    "\n",
    "Заметим, что $R^2$ при добавлении любых фичей только растёт, так что сравнивать модели по нему некорректно, однако с этим борятся, используя несмещённую оценку дисперсии:\n",
    "\n",
    "$\\sigma_t^2 = \\frac{1}{n-1}\\sum (y - \\overline{y})^2$\n",
    "\n",
    "$\\sigma_y^2 = \\frac{1}{n-k}\\sum (\\hat{y} - \\overline{y})^2$, где $\\hat{y} = w^TX$\n",
    "\n",
    "$R_{adj}^2 = 1 - \\frac{\\sigma^2}{\\sigma_t^2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0mmLUaCMmcd"
   },
   "source": [
    "Посмотрим, как изменятся $R^2$ и adjusted $R^2$ если мы поудаляем фичи."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aLe9E9rqCxVS",
    "outputId": "2b3cdf2d-71ad-4b3a-fc32-09051748a76a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.750\n",
      "Model:                            OLS   Adj. R-squared:                  0.749\n",
      "Method:                 Least Squares   F-statistic:                     998.1\n",
      "Date:                Sat, 09 Nov 2024   Prob (F-statistic):               0.00\n",
      "Time:                        10:19:45   Log-Likelihood:                -13551.\n",
      "No. Observations:                1338   AIC:                         2.711e+04\n",
      "Df Residuals:                    1333   BIC:                         2.714e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1           257.8495     11.896     21.675      0.000     234.512     281.187\n",
      "x2           321.8514     27.378     11.756      0.000     268.143     375.559\n",
      "x3           473.5023    137.792      3.436      0.001     203.190     743.814\n",
      "x4         -1.197e+04    354.354    -33.784      0.000   -1.27e+04   -1.13e+04\n",
      "x5          1.184e+04    396.710     29.846      0.000    1.11e+04    1.26e+04\n",
      "const       -131.3796    629.912     -0.209      0.835   -1367.107    1104.348\n",
      "==============================================================================\n",
      "Omnibus:                      301.480   Durbin-Watson:                   2.087\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              722.157\n",
      "Skew:                           1.215   Prob(JB):                    1.53e-157\n",
      "Kurtosis:                       5.654   Cond. No.                     1.32e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.02e-28. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "X_new = X[:, [0, 1, 2, 9, 10, 11]]\n",
    "mod = sm.OLS(Y, X_new)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H8I5Iq4ZM5xX",
    "outputId": "daae73bf-df3e-44bc-ac31-fb8f0f63f5a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6056.439217188081"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((Y - X_new@sp.linalg.pinv(X_new)@Y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rLnSul3NH9w",
    "outputId": "ef3ac71a-5244-4bed-9493-21b057b37283"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.750\n",
      "Model:                            OLS   Adj. R-squared:                  0.749\n",
      "Method:                 Least Squares   F-statistic:                     998.1\n",
      "Date:                Sat, 09 Nov 2024   Prob (F-statistic):               0.00\n",
      "Time:                        10:19:46   Log-Likelihood:                -13551.\n",
      "No. Observations:                1338   AIC:                         2.711e+04\n",
      "Df Residuals:                    1333   BIC:                         2.714e+04\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1           257.8495     11.896     21.675      0.000     234.512     281.187\n",
      "x2           321.8514     27.378     11.756      0.000     268.143     375.559\n",
      "x3           473.5023    137.792      3.436      0.001     203.190     743.814\n",
      "x4         -2.381e+04    411.220    -57.904      0.000   -2.46e+04    -2.3e+04\n",
      "const       1.171e+04    991.347     11.811      0.000    9763.860    1.37e+04\n",
      "==============================================================================\n",
      "Omnibus:                      301.480   Durbin-Watson:                   2.087\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              722.157\n",
      "Skew:                           1.215   Prob(JB):                    1.53e-157\n",
      "Kurtosis:                       5.654   Cond. No.                         310.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "X_new = X[:, [0, 1, 2, 9, 11]]\n",
    "mod = sm.OLS(Y, X_new)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUtI5y9NOV5k",
    "outputId": "7713bbe9-7590-4345-ff07-e51a77c0835d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6056.439217188082"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((Y - X_new@sp.linalg.pinv(X_new)@Y)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6qra_QLOmL_",
    "outputId": "6b586b77-baa6-4b15-aed2-d8c8f0b36acb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.120\n",
      "Model:                            OLS   Adj. R-squared:                  0.118\n",
      "Method:                 Least Squares   F-statistic:                     60.69\n",
      "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           8.80e-37\n",
      "Time:                        10:19:47   Log-Likelihood:                -14392.\n",
      "No. Observations:                1338   AIC:                         2.879e+04\n",
      "Df Residuals:                    1334   BIC:                         2.881e+04\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1           239.9945     22.289     10.767      0.000     196.269     283.720\n",
      "x2           332.0834     51.310      6.472      0.000     231.425     432.741\n",
      "x3           542.8647    258.241      2.102      0.036      36.261    1049.468\n",
      "const      -6916.2433   1757.480     -3.935      0.000   -1.04e+04   -3468.518\n",
      "==============================================================================\n",
      "Omnibus:                      325.395   Durbin-Watson:                   2.012\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              603.372\n",
      "Skew:                           1.520   Prob(JB):                    9.54e-132\n",
      "Kurtosis:                       4.255   Cond. No.                         290.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "X_new = X[:, [0, 1, 2, 11]]\n",
    "mod = sm.OLS(Y, X_new)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JQfxpIWsPfz0",
    "outputId": "51eac180-7bd3-4a1b-89db-a6be5af05015"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11355.317901125973"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sqrt(np.mean((Y - X_new@sp.linalg.pinv(X_new)@Y)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PDNUx8w4PinQ"
   },
   "source": [
    "Обратите внимание на то, как резко упал $R^2$ от удаления признака 9. Значит, он был важным."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wgd1sIjqQ8px"
   },
   "source": [
    "Чуть улучшенными версиями коэффициента детерминации являются информационный критерий Акаике AIC и байесовский информационный критерий Шварца BIC.\n",
    "\n",
    "$AIC = k - ln(L)$, где $k$ это число параметров, а $L$ это достигаемое моделью правдоподобие. То есть мы хотим, чтобы модель как можно точнее предсказала имеющиеся данные за как можно меньше параметров.\n",
    "\n",
    "$BIC = kln(n) - ln(L)$, $n$ --- количество наблюдений $X, Y$. Заметим, что байесовский критерий сильнее штрафует модель за рост числа признаков.\n",
    "\n",
    "В отличии от $R^2$ информационные критерии не имеют вероятностной интерпретации своих значений и могут использоваться только для сравнения, причём чем информационный критерий меньше, тем модель лучше.\n",
    "\n",
    "НЕ ДЕЛАЙТЕ ВЫВОДОВ НА ОСНОВАНИИ АБСОЛЮТНЫХ ЗНАЧЕНИЙ ИНФОРМАЦИОННЫХ КРИТЕРИЕВ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "snvsxFI3TPxU"
   },
   "source": [
    "Сравним признаки 1 (возраст) и 10 (курение)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j_TXhJnIP571",
    "outputId": "a6ca44b0-b823-41dd-c085-a199497c9294"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.126\n",
      "Model:                            OLS   Adj. R-squared:                  0.122\n",
      "Method:                 Least Squares   F-statistic:                     27.50\n",
      "Date:                Sat, 09 Nov 2024   Prob (F-statistic):           1.86e-35\n",
      "Time:                        10:20:09   Log-Likelihood:                -14387.\n",
      "No. Observations:                1338   AIC:                         2.879e+04\n",
      "Df Residuals:                    1330   BIC:                         2.883e+04\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1           242.2542     22.270     10.878      0.000     198.566     285.942\n",
      "x2           314.5987     53.533      5.877      0.000     209.580     419.617\n",
      "x3           555.6912    257.960      2.154      0.031      49.638    1061.744\n",
      "x4         -2515.7522    593.699     -4.237      0.000   -3680.441   -1351.063\n",
      "x5         -1204.1443    609.625     -1.975      0.048   -2400.076      -8.212\n",
      "x6          -290.1464    578.539     -0.502      0.616   -1425.095     844.802\n",
      "x7         -1316.1225    579.576     -2.271      0.023   -2453.105    -179.141\n",
      "x8          -220.1606    650.237     -0.339      0.735   -1495.763    1055.441\n",
      "x9         -1893.4669    603.375     -3.138      0.002   -3077.137    -709.796\n",
      "const      -3719.8965   1030.511     -3.610      0.000   -5741.500   -1698.293\n",
      "==============================================================================\n",
      "Omnibus:                      322.510   Durbin-Watson:                   2.025\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              595.783\n",
      "Skew:                           1.507   Prob(JB):                    4.24e-130\n",
      "Kurtosis:                       4.268   Cond. No.                     3.00e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.91e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "X_new = X[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 11]]\n",
    "mod = sm.OLS(Y, X_new)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kbtu2QY2SjYm",
    "outputId": "a5b4fe31-d25f-4209-d227-805dbbd05343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.664\n",
      "Model:                            OLS   Adj. R-squared:                  0.662\n",
      "Method:                 Least Squares   F-statistic:                     374.8\n",
      "Date:                Sat, 09 Nov 2024   Prob (F-statistic):          2.97e-309\n",
      "Time:                        10:19:48   Log-Likelihood:                -13749.\n",
      "No. Observations:                1338   AIC:                         2.751e+04\n",
      "Df Residuals:                    1330   BIC:                         2.756e+04\n",
      "Df Model:                           7                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1           411.2700     32.998     12.464      0.000     346.537     476.003\n",
      "x2           597.4632    159.956      3.735      0.000     283.669     911.257\n",
      "x3          5538.8242    366.889     15.097      0.000    4819.080    6258.569\n",
      "x4          5227.9733    371.897     14.058      0.000    4498.404    5957.543\n",
      "x5          3400.3317    356.644      9.534      0.000    2700.686    4099.977\n",
      "x6          3007.9632    359.814      8.360      0.000    2302.099    3713.828\n",
      "x7          1990.0624    402.001      4.950      0.000    1201.438    2778.687\n",
      "x8          2368.4402    374.765      6.320      0.000    1633.246    3103.634\n",
      "x9         -2.366e+04    479.863    -49.303      0.000   -2.46e+04   -2.27e+04\n",
      "const       1.077e+04    629.536     17.103      0.000    9531.805     1.2e+04\n",
      "==============================================================================\n",
      "Omnibus:                      162.572   Durbin-Watson:                   2.043\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              258.818\n",
      "Skew:                           0.833   Prob(JB):                     6.28e-57\n",
      "Kurtosis:                       4.366   Cond. No.                     2.50e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 2.1e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "X_new = X[:, [1, 2, 3, 4, 5, 6, 7, 8, 9, 11]]\n",
    "mod = sm.OLS(Y, X_new)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQ1PGKC5SoH5",
    "outputId": "ce8a42ff-9c63-4fad-dd6a-3247801213e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                      y   R-squared:                       0.751\n",
      "Model:                            OLS   Adj. R-squared:                  0.749\n",
      "Method:                 Least Squares   F-statistic:                     500.8\n",
      "Date:                Sat, 09 Nov 2024   Prob (F-statistic):               0.00\n",
      "Time:                        10:19:49   Log-Likelihood:                -13548.\n",
      "No. Observations:                1338   AIC:                         2.711e+04\n",
      "Df Residuals:                    1329   BIC:                         2.716e+04\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "x1           256.8564     11.899     21.587      0.000     233.514     280.199\n",
      "x2           339.1935     28.599     11.860      0.000     283.088     395.298\n",
      "x3           475.5005    137.804      3.451      0.001     205.163     745.838\n",
      "x4          3282.0371    332.668      9.866      0.000    2629.426    3934.648\n",
      "x5          3150.7227    334.275      9.426      0.000    2494.959    3806.486\n",
      "x6          2195.1992    312.029      7.035      0.000    1583.076    2807.322\n",
      "x7          1842.2353    314.396      5.860      0.000    1225.470    2459.001\n",
      "x8          1160.1771    348.166      3.332      0.001     477.162    1843.192\n",
      "x9          1235.1482    326.836      3.779      0.000     593.977    1876.320\n",
      "x10        -2.385e+04    413.153    -57.723      0.000   -2.47e+04    -2.3e+04\n",
      "const       6432.7597    577.894     11.131      0.000    5299.075    7566.444\n",
      "==============================================================================\n",
      "Omnibus:                      300.366   Durbin-Watson:                   2.088\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):              718.887\n",
      "Skew:                           1.211   Prob(JB):                    7.86e-157\n",
      "Kurtosis:                       5.651   Cond. No.                     3.27e+17\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The smallest eigenvalue is 3.28e-29. This might indicate that there are\n",
      "strong multicollinearity problems or that the design matrix is singular.\n"
     ]
    }
   ],
   "source": [
    "X_new = X[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 11]]\n",
    "mod = sm.OLS(Y, X_new)\n",
    "res = mod.fit()\n",
    "print(res.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iTRGnlgiSv2F"
   },
   "source": [
    "AIC модели с всеми признаками: 2.711e+04\n",
    "\n",
    "AIC модели без признака 1: 2.751e+04\n",
    "\n",
    "AIC модели без признака 10: 2.879e+04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Av4GBMAkTH0o"
   },
   "source": [
    "Оба признака важны, но признак 10 важнее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gT1nRjB5TVb0"
   },
   "source": [
    "Как мы уже поняли, с вероятностной и информационной точки зрения признаки можно сортиовать по важности и отбирать нужные нам. Обычно признаки, у которых вероятность $t$-статистики больше 0.05 просто выбрасывают и не рассматривают, однако стоит помнить, что признаки обычно зависимы и их полезность не может рассматриваться отдельно, только при условии остальных признаков. Признак 1 может быть неважным, пока есть признак 2 и резко стать важным, если признак 2 удалить. Поэтому приходится заниматься полным перебором подмножеств признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4q7b7VwyUWFG"
   },
   "source": [
    "2. Если вы знаете алгебру, то понимаете, что наличие зависимых признаков ухудшает свойства задачи и добавляет численные ошибки. Однако нельзя удалять признаки так, чтобы оставшиеся были независимыми --- вполне может оказаться, что есть 3-5 полезных сильно зависимых друг от друга признаков. Один из подходов для борьбы с **мультиколлинеарностью** признаков является сжатие данных PCA, о котором вам расскажут на машинном обучении, но мы рассмотрим другой подход --- регуляризацию.\n",
    "\n",
    "А чем так плох PCA? Основная проблема --- потеря интерпретируемости признаков. Компоненты PCA не имеют никакого прикладного смысла."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Z9_SCU9Vdoy"
   },
   "source": [
    "Добавляем априорное распределение к модели $Y \\sim N(w^TX, \\beta^{-1}I)$. Проверьте, что при оценке мат. ожидания нормального распределения сопряжённым являтся снова нормальное распределение. Так что возьмём $w \\sim N(0, \\lambda^{-1}I)$. Тогда можно сделать апостериорный байесовский вывод (или оценку байесом для бедных, так как мы знаем, что апостериорная плотность будет нормальной, а нормальная плотность определяется своими мат. ожиданием и дисперсией. Мат. ожиданием и будет оценка байеса для бедных, а дисперсия это какая-то вычисляемая константа, предлагаем вам вычислить самим).\n",
    "\n",
    "Проверьте, что $w^* = (\\beta X^TX + \\lambda I)^{-1}X^TY$. Как изменились алгебраические свойства обращаемой матрицы?\n",
    "\n",
    "Изучим поведение модели в зависимости от константы регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NXK4Gq24Xgw_"
   },
   "source": [
    "В экономике такая модель называется Ridge regression. Иногда вместо нормального праера берут Лаплассовский, который соответствует регуляризации $\\lambda|w|$ (сумма не с квадратом, а с модулем). Такая модель называется Lasso. А иногда и произведение праеров. Последняя модель называется Elastic net. Последняя модель до сих пор является SOTA решением в экономических моделях благодаря высокой интерпретируемости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dz7bhGAgYCJn"
   },
   "source": [
    "$\\alpha$ это общий параметр регуляризации, а L1_wt это доля нормального праера в сумме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wS8JrC_CSrQh",
    "outputId": "c6f4e2d8-f91d-408a-e9fb-9f2534b66ea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-35   6101.138009089193\n",
      "1e-20   6041.679651174447\n",
      "1e-09   6041.6796511744515\n",
      "1e-06   6041.679651247595\n",
      "1e-05   6041.67965848828\n",
      "0.0001   6041.680382140012\n",
      "0.0001   6041.680382140012\n",
      "0.001   6041.752332085743\n",
      "0.01   6048.548206338384\n",
      "0.1   6451.72634183886\n",
      "0   6104.187394630987\n",
      "1   9457.417540627457\n",
      "10   11165.422607624352\n",
      "100   11416.562305616884\n",
      "1000   12051.359310264907\n"
     ]
    }
   ],
   "source": [
    "X_new = X[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]\n",
    "for i in [1e-35, 1e-20, 1e-9, 1e-6, 1e-5,1e-4, 0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000]:\n",
    "    mod = sm.OLS(Y, X_new)\n",
    "    res = mod.fit_regularized(alpha=i, L1_wt=0,refit=True)\n",
    "    print(i,' ',np.sqrt(np.mean((Y - res.predict(X_new))**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f4Jjq-VTZbj2"
   },
   "source": [
    "Как мы видим, регуляризация повышает качество даже на тренировочной выборке за счёт улучшения свойств обращаемых матриц. А с курса машинного обучения вы знаете, что оно улучшает ещё и генерализацию (качество предсказания новых данных)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eAsrhHjaZ0u2"
   },
   "source": [
    "Нам в статистике больше интересна не Ridge регуляризация, которая делает модель устойчивее к шуму в данных (как мы обсудили на семинаре про байес), а Lasso, которое обладает острым пиком (в отличии от гладкого гауссовского). На практике это значит, что это распределение существенно убывает при отходе от пика. То есть модель предпочтёт $w = 0$, а не выбор какого-то маленького $w$. Можете посмотреть на задачу оптимизации байеса для бедных:\n",
    "\n",
    "$w^* = argmin ||Y - w^TX||^2 + \\lambda |w|$.\n",
    "\n",
    "Её градиент не убывает при подходе к решению, так как содержит константный член в сумме. Это позволяет сойтись не в окрестность решения, а точно к нему.\n",
    "\n",
    "Мы же будем использовать лассо для отбора признаков. Если лассо решил ставить признак равным нулю, то он не нужен.\n",
    "\n",
    "Заметим, что можно использовать как лассо модель, так и обучать её только промежутночно: обучить, выбросить фичи с нулевым весом, обучить модель без регуляризации или с Ridge, так как в таком случае точность будет выше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XvL2wM26YMxD",
    "outputId": "57391ed6-f668-432a-b6f1-beca89159186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-35   6041.6796511744515\n",
      "1e-20   6041.6796511744515\n",
      "1e-09   6041.6796511744515\n",
      "1e-06   6041.6796511744515\n",
      "1e-05   6041.6796511744515\n",
      "0.0001   6041.6796511744515\n",
      "0.0001   6041.6796511744515\n",
      "0.001   6041.6796511744515\n",
      "0.01   6041.6796511744515\n",
      "0.1   6041.6796511744515\n",
      "0   6041.6796511744515\n",
      "1   6041.6796511744515\n",
      "10   6046.287338312512\n",
      "100   6056.439217188081\n",
      "1000   6083.206042088949\n"
     ]
    }
   ],
   "source": [
    "X_new = X[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]\n",
    "for i in [1e-35, 1e-20, 1e-9, 1e-6, 1e-5,1e-4, 0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000]:\n",
    "    mod = sm.OLS(Y, X_new)\n",
    "    res = mod.fit_regularized(alpha=i, L1_wt=1,refit=True)\n",
    "    print(i,' ',np.sqrt(np.mean((Y - res.predict(X_new))**2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3XLlwdEa7sG",
    "outputId": "12b9bc83-9f98-462a-824c-ade205b98fcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   259.54749155    322.61513282      0.              0.\n",
      "      0.              0.              0.              0.\n",
      "      0.         -11676.83042519  12146.85407012      0.        ]\n"
     ]
    }
   ],
   "source": [
    "res = mod.fit_regularized(alpha=i, L1_wt=1,refit=True)\n",
    "print(res.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TrwFiJgtbGoF"
   },
   "source": [
    "Модель фильтрует фичи 3-9 и последний. Последний это добавленная нами константа, а фичи 3-9 были признаны бесполезными ещё на основании $t$-статистики, то есть противоречия нет. Однако, стоит отметить, что лассо регрессия существенно агрессивнее, чем классические подходы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W1UWMdrTbFj3",
    "outputId": "0a6b90ce-2d35-4615-b865-7290a0ddc3a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1e-35   1\n",
      "1e-20   1\n",
      "1e-09   1\n",
      "1e-06   1\n",
      "1e-05   1\n",
      "0.0001   1\n",
      "0.0001   1\n",
      "0.001   1\n",
      "0.01   1\n",
      "0.1   1\n",
      "0   1\n",
      "1   2\n",
      "10   3\n",
      "100   7\n",
      "1000   8\n"
     ]
    }
   ],
   "source": [
    "X_new = X[:, [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]]\n",
    "for i in [1e-35, 1e-20, 1e-9, 1e-6, 1e-5,1e-4, 0.0001, 0.001, 0.01, 0.1, 0, 1, 10, 100, 1000]:\n",
    "    mod = sm.OLS(Y, X_new)\n",
    "    res = mod.fit_regularized(alpha=i, L1_wt=1,refit=True)\n",
    "    print(i,' ',(res.params==0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9b_MphacfDL"
   },
   "source": [
    "А агрессивность настраиваится при помощи параметра регуляризации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z4ptErykcmri"
   },
   "source": [
    "3. Байесовский подход.\n",
    "\n",
    "Заметим, что с ростом лямбды оценка байеса для бедных сходится к нулю, а при приближении лябмда к нулю оценка сходится к MLE. Отсюда следует, что стоит брать в качестве праера не $\\lambda^{-1} I$, а диагональную матрицу, где для полезных признаков мы выставим небольшую регуляризацию, а для бесполезных большую. То есть параметр регуляризации можно использовать как меру полезности признака, но пока что мы откуда-то знаем полезность и подбираем регуляризацию.\n",
    "\n",
    "А что если попробовать наоборот? Мы с прошлого семинара уже научились подбирать модель исходя из принципа наибольшей обоснованности. Давайте параметризуем праеры векторами регуляризации $\\lambda$, стоящими на диагонали матрицы ковариаций априорного распределения и подберём эти лямбды из принципа наибольшего правдоподобия. Этот метод называется **методом релевантных векторов** и описан в лекции 4 конспекта Д.П.Ветрова по байесовскому анализу.\n",
    "\n",
    "Так как поля этого ноутбука (и семинара) слишком узки, мы предлагаем разобраться (и задать в чате вопросы) в реализации этого метода самим. Если коротко --- нужно выписать правдоподобие как функцию от $w^*$ и $\\lambda$ и максимизировать по лямбда."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2sgm1l2meUsS"
   },
   "source": [
    "4. Теоретико-игровой подход (что?).\n",
    "\n",
    "Заметим, что регрессионную модель можно рассматривать как коалиционную игру, в которой признаки образуют коалиции и зарабатывают $R^2$ коэффициент. В силу свойств $R^2$ коэффициента эта игра оказывается ещё и супераддитивной, так что можно \"разделить\" награду $R^2$ на все фичи по **вектору Шепли** (что это такое рассказывают на теории игр). И трактовать долю каждого признака как полезность."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7MgqDbhcX3I"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "stats_kernel",
   "language": "python",
   "name": "stats_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
